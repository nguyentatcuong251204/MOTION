{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /home/hongn/sapiens/pretrain/checkpoints/sapiens_0.3b/sapiens_0.3b_epoch_1600_clean.pth on server: t1v-n-3c3909ab-w-0\n",
      "Done: Loaded checkpoint from /home/hongn/sapiens/pretrain/checkpoints/sapiens_0.3b/sapiens_0.3b_epoch_1600_clean.pth on server: t1v-n-3c3909ab-w-0\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: cls_token, pos_embed, patch_embed.projection.weight, patch_embed.projection.bias, layers.0.ln1.weight, layers.0.ln1.bias, layers.0.attn.qkv.weight, layers.0.attn.qkv.bias, layers.0.attn.proj.weight, layers.0.attn.proj.bias, layers.0.ln2.weight, layers.0.ln2.bias, layers.0.ffn.layers.0.0.weight, layers.0.ffn.layers.0.0.bias, layers.0.ffn.layers.1.weight, layers.0.ffn.layers.1.bias, layers.1.ln1.weight, layers.1.ln1.bias, layers.1.attn.qkv.weight, layers.1.attn.qkv.bias, layers.1.attn.proj.weight, layers.1.attn.proj.bias, layers.1.ln2.weight, layers.1.ln2.bias, layers.1.ffn.layers.0.0.weight, layers.1.ffn.layers.0.0.bias, layers.1.ffn.layers.1.weight, layers.1.ffn.layers.1.bias, layers.2.ln1.weight, layers.2.ln1.bias, layers.2.attn.qkv.weight, layers.2.attn.qkv.bias, layers.2.attn.proj.weight, layers.2.attn.proj.bias, layers.2.ln2.weight, layers.2.ln2.bias, layers.2.ffn.layers.0.0.weight, layers.2.ffn.layers.0.0.bias, layers.2.ffn.layers.1.weight, layers.2.ffn.layers.1.bias, layers.3.ln1.weight, layers.3.ln1.bias, layers.3.attn.qkv.weight, layers.3.attn.qkv.bias, layers.3.attn.proj.weight, layers.3.attn.proj.bias, layers.3.ln2.weight, layers.3.ln2.bias, layers.3.ffn.layers.0.0.weight, layers.3.ffn.layers.0.0.bias, layers.3.ffn.layers.1.weight, layers.3.ffn.layers.1.bias, layers.4.ln1.weight, layers.4.ln1.bias, layers.4.attn.qkv.weight, layers.4.attn.qkv.bias, layers.4.attn.proj.weight, layers.4.attn.proj.bias, layers.4.ln2.weight, layers.4.ln2.bias, layers.4.ffn.layers.0.0.weight, layers.4.ffn.layers.0.0.bias, layers.4.ffn.layers.1.weight, layers.4.ffn.layers.1.bias, layers.5.ln1.weight, layers.5.ln1.bias, layers.5.attn.qkv.weight, layers.5.attn.qkv.bias, layers.5.attn.proj.weight, layers.5.attn.proj.bias, layers.5.ln2.weight, layers.5.ln2.bias, layers.5.ffn.layers.0.0.weight, layers.5.ffn.layers.0.0.bias, layers.5.ffn.layers.1.weight, layers.5.ffn.layers.1.bias, layers.6.ln1.weight, layers.6.ln1.bias, layers.6.attn.qkv.weight, layers.6.attn.qkv.bias, layers.6.attn.proj.weight, layers.6.attn.proj.bias, layers.6.ln2.weight, layers.6.ln2.bias, layers.6.ffn.layers.0.0.weight, layers.6.ffn.layers.0.0.bias, layers.6.ffn.layers.1.weight, layers.6.ffn.layers.1.bias, layers.7.ln1.weight, layers.7.ln1.bias, layers.7.attn.qkv.weight, layers.7.attn.qkv.bias, layers.7.attn.proj.weight, layers.7.attn.proj.bias, layers.7.ln2.weight, layers.7.ln2.bias, layers.7.ffn.layers.0.0.weight, layers.7.ffn.layers.0.0.bias, layers.7.ffn.layers.1.weight, layers.7.ffn.layers.1.bias, layers.8.ln1.weight, layers.8.ln1.bias, layers.8.attn.qkv.weight, layers.8.attn.qkv.bias, layers.8.attn.proj.weight, layers.8.attn.proj.bias, layers.8.ln2.weight, layers.8.ln2.bias, layers.8.ffn.layers.0.0.weight, layers.8.ffn.layers.0.0.bias, layers.8.ffn.layers.1.weight, layers.8.ffn.layers.1.bias, layers.9.ln1.weight, layers.9.ln1.bias, layers.9.attn.qkv.weight, layers.9.attn.qkv.bias, layers.9.attn.proj.weight, layers.9.attn.proj.bias, layers.9.ln2.weight, layers.9.ln2.bias, layers.9.ffn.layers.0.0.weight, layers.9.ffn.layers.0.0.bias, layers.9.ffn.layers.1.weight, layers.9.ffn.layers.1.bias, layers.10.ln1.weight, layers.10.ln1.bias, layers.10.attn.qkv.weight, layers.10.attn.qkv.bias, layers.10.attn.proj.weight, layers.10.attn.proj.bias, layers.10.ln2.weight, layers.10.ln2.bias, layers.10.ffn.layers.0.0.weight, layers.10.ffn.layers.0.0.bias, layers.10.ffn.layers.1.weight, layers.10.ffn.layers.1.bias, layers.11.ln1.weight, layers.11.ln1.bias, layers.11.attn.qkv.weight, layers.11.attn.qkv.bias, layers.11.attn.proj.weight, layers.11.attn.proj.bias, layers.11.ln2.weight, layers.11.ln2.bias, layers.11.ffn.layers.0.0.weight, layers.11.ffn.layers.0.0.bias, layers.11.ffn.layers.1.weight, layers.11.ffn.layers.1.bias, layers.12.ln1.weight, layers.12.ln1.bias, layers.12.attn.qkv.weight, layers.12.attn.qkv.bias, layers.12.attn.proj.weight, layers.12.attn.proj.bias, layers.12.ln2.weight, layers.12.ln2.bias, layers.12.ffn.layers.0.0.weight, layers.12.ffn.layers.0.0.bias, layers.12.ffn.layers.1.weight, layers.12.ffn.layers.1.bias, layers.13.ln1.weight, layers.13.ln1.bias, layers.13.attn.qkv.weight, layers.13.attn.qkv.bias, layers.13.attn.proj.weight, layers.13.attn.proj.bias, layers.13.ln2.weight, layers.13.ln2.bias, layers.13.ffn.layers.0.0.weight, layers.13.ffn.layers.0.0.bias, layers.13.ffn.layers.1.weight, layers.13.ffn.layers.1.bias, layers.14.ln1.weight, layers.14.ln1.bias, layers.14.attn.qkv.weight, layers.14.attn.qkv.bias, layers.14.attn.proj.weight, layers.14.attn.proj.bias, layers.14.ln2.weight, layers.14.ln2.bias, layers.14.ffn.layers.0.0.weight, layers.14.ffn.layers.0.0.bias, layers.14.ffn.layers.1.weight, layers.14.ffn.layers.1.bias, layers.15.ln1.weight, layers.15.ln1.bias, layers.15.attn.qkv.weight, layers.15.attn.qkv.bias, layers.15.attn.proj.weight, layers.15.attn.proj.bias, layers.15.ln2.weight, layers.15.ln2.bias, layers.15.ffn.layers.0.0.weight, layers.15.ffn.layers.0.0.bias, layers.15.ffn.layers.1.weight, layers.15.ffn.layers.1.bias, layers.16.ln1.weight, layers.16.ln1.bias, layers.16.attn.qkv.weight, layers.16.attn.qkv.bias, layers.16.attn.proj.weight, layers.16.attn.proj.bias, layers.16.ln2.weight, layers.16.ln2.bias, layers.16.ffn.layers.0.0.weight, layers.16.ffn.layers.0.0.bias, layers.16.ffn.layers.1.weight, layers.16.ffn.layers.1.bias, layers.17.ln1.weight, layers.17.ln1.bias, layers.17.attn.qkv.weight, layers.17.attn.qkv.bias, layers.17.attn.proj.weight, layers.17.attn.proj.bias, layers.17.ln2.weight, layers.17.ln2.bias, layers.17.ffn.layers.0.0.weight, layers.17.ffn.layers.0.0.bias, layers.17.ffn.layers.1.weight, layers.17.ffn.layers.1.bias, layers.18.ln1.weight, layers.18.ln1.bias, layers.18.attn.qkv.weight, layers.18.attn.qkv.bias, layers.18.attn.proj.weight, layers.18.attn.proj.bias, layers.18.ln2.weight, layers.18.ln2.bias, layers.18.ffn.layers.0.0.weight, layers.18.ffn.layers.0.0.bias, layers.18.ffn.layers.1.weight, layers.18.ffn.layers.1.bias, layers.19.ln1.weight, layers.19.ln1.bias, layers.19.attn.qkv.weight, layers.19.attn.qkv.bias, layers.19.attn.proj.weight, layers.19.attn.proj.bias, layers.19.ln2.weight, layers.19.ln2.bias, layers.19.ffn.layers.0.0.weight, layers.19.ffn.layers.0.0.bias, layers.19.ffn.layers.1.weight, layers.19.ffn.layers.1.bias, layers.20.ln1.weight, layers.20.ln1.bias, layers.20.attn.qkv.weight, layers.20.attn.qkv.bias, layers.20.attn.proj.weight, layers.20.attn.proj.bias, layers.20.ln2.weight, layers.20.ln2.bias, layers.20.ffn.layers.0.0.weight, layers.20.ffn.layers.0.0.bias, layers.20.ffn.layers.1.weight, layers.20.ffn.layers.1.bias, layers.21.ln1.weight, layers.21.ln1.bias, layers.21.attn.qkv.weight, layers.21.attn.qkv.bias, layers.21.attn.proj.weight, layers.21.attn.proj.bias, layers.21.ln2.weight, layers.21.ln2.bias, layers.21.ffn.layers.0.0.weight, layers.21.ffn.layers.0.0.bias, layers.21.ffn.layers.1.weight, layers.21.ffn.layers.1.bias, layers.22.ln1.weight, layers.22.ln1.bias, layers.22.attn.qkv.weight, layers.22.attn.qkv.bias, layers.22.attn.proj.weight, layers.22.attn.proj.bias, layers.22.ln2.weight, layers.22.ln2.bias, layers.22.ffn.layers.0.0.weight, layers.22.ffn.layers.0.0.bias, layers.22.ffn.layers.1.weight, layers.22.ffn.layers.1.bias, layers.23.ln1.weight, layers.23.ln1.bias, layers.23.attn.qkv.weight, layers.23.attn.qkv.bias, layers.23.attn.proj.weight, layers.23.attn.proj.bias, layers.23.ln2.weight, layers.23.ln2.bias, layers.23.ffn.layers.0.0.weight, layers.23.ffn.layers.0.0.bias, layers.23.ffn.layers.1.weight, layers.23.ffn.layers.1.bias, ln1.weight, ln1.bias\n",
      "\n",
      "missing keys in source state_dict: backbone.cls_token, backbone.pos_embed, backbone.patch_embed.projection.weight, backbone.patch_embed.projection.bias, backbone.layers.0.ln1.weight, backbone.layers.0.ln1.bias, backbone.layers.0.attn.qkv.weight, backbone.layers.0.attn.qkv.bias, backbone.layers.0.attn.proj.weight, backbone.layers.0.attn.proj.bias, backbone.layers.0.ln2.weight, backbone.layers.0.ln2.bias, backbone.layers.0.ffn.layers.0.0.weight, backbone.layers.0.ffn.layers.0.0.bias, backbone.layers.0.ffn.layers.1.weight, backbone.layers.0.ffn.layers.1.bias, backbone.layers.1.ln1.weight, backbone.layers.1.ln1.bias, backbone.layers.1.attn.qkv.weight, backbone.layers.1.attn.qkv.bias, backbone.layers.1.attn.proj.weight, backbone.layers.1.attn.proj.bias, backbone.layers.1.ln2.weight, backbone.layers.1.ln2.bias, backbone.layers.1.ffn.layers.0.0.weight, backbone.layers.1.ffn.layers.0.0.bias, backbone.layers.1.ffn.layers.1.weight, backbone.layers.1.ffn.layers.1.bias, backbone.layers.2.ln1.weight, backbone.layers.2.ln1.bias, backbone.layers.2.attn.qkv.weight, backbone.layers.2.attn.qkv.bias, backbone.layers.2.attn.proj.weight, backbone.layers.2.attn.proj.bias, backbone.layers.2.ln2.weight, backbone.layers.2.ln2.bias, backbone.layers.2.ffn.layers.0.0.weight, backbone.layers.2.ffn.layers.0.0.bias, backbone.layers.2.ffn.layers.1.weight, backbone.layers.2.ffn.layers.1.bias, backbone.layers.3.ln1.weight, backbone.layers.3.ln1.bias, backbone.layers.3.attn.qkv.weight, backbone.layers.3.attn.qkv.bias, backbone.layers.3.attn.proj.weight, backbone.layers.3.attn.proj.bias, backbone.layers.3.ln2.weight, backbone.layers.3.ln2.bias, backbone.layers.3.ffn.layers.0.0.weight, backbone.layers.3.ffn.layers.0.0.bias, backbone.layers.3.ffn.layers.1.weight, backbone.layers.3.ffn.layers.1.bias, backbone.layers.4.ln1.weight, backbone.layers.4.ln1.bias, backbone.layers.4.attn.qkv.weight, backbone.layers.4.attn.qkv.bias, backbone.layers.4.attn.proj.weight, backbone.layers.4.attn.proj.bias, backbone.layers.4.ln2.weight, backbone.layers.4.ln2.bias, backbone.layers.4.ffn.layers.0.0.weight, backbone.layers.4.ffn.layers.0.0.bias, backbone.layers.4.ffn.layers.1.weight, backbone.layers.4.ffn.layers.1.bias, backbone.layers.5.ln1.weight, backbone.layers.5.ln1.bias, backbone.layers.5.attn.qkv.weight, backbone.layers.5.attn.qkv.bias, backbone.layers.5.attn.proj.weight, backbone.layers.5.attn.proj.bias, backbone.layers.5.ln2.weight, backbone.layers.5.ln2.bias, backbone.layers.5.ffn.layers.0.0.weight, backbone.layers.5.ffn.layers.0.0.bias, backbone.layers.5.ffn.layers.1.weight, backbone.layers.5.ffn.layers.1.bias, backbone.layers.6.ln1.weight, backbone.layers.6.ln1.bias, backbone.layers.6.attn.qkv.weight, backbone.layers.6.attn.qkv.bias, backbone.layers.6.attn.proj.weight, backbone.layers.6.attn.proj.bias, backbone.layers.6.ln2.weight, backbone.layers.6.ln2.bias, backbone.layers.6.ffn.layers.0.0.weight, backbone.layers.6.ffn.layers.0.0.bias, backbone.layers.6.ffn.layers.1.weight, backbone.layers.6.ffn.layers.1.bias, backbone.layers.7.ln1.weight, backbone.layers.7.ln1.bias, backbone.layers.7.attn.qkv.weight, backbone.layers.7.attn.qkv.bias, backbone.layers.7.attn.proj.weight, backbone.layers.7.attn.proj.bias, backbone.layers.7.ln2.weight, backbone.layers.7.ln2.bias, backbone.layers.7.ffn.layers.0.0.weight, backbone.layers.7.ffn.layers.0.0.bias, backbone.layers.7.ffn.layers.1.weight, backbone.layers.7.ffn.layers.1.bias, backbone.layers.8.ln1.weight, backbone.layers.8.ln1.bias, backbone.layers.8.attn.qkv.weight, backbone.layers.8.attn.qkv.bias, backbone.layers.8.attn.proj.weight, backbone.layers.8.attn.proj.bias, backbone.layers.8.ln2.weight, backbone.layers.8.ln2.bias, backbone.layers.8.ffn.layers.0.0.weight, backbone.layers.8.ffn.layers.0.0.bias, backbone.layers.8.ffn.layers.1.weight, backbone.layers.8.ffn.layers.1.bias, backbone.layers.9.ln1.weight, backbone.layers.9.ln1.bias, backbone.layers.9.attn.qkv.weight, backbone.layers.9.attn.qkv.bias, backbone.layers.9.attn.proj.weight, backbone.layers.9.attn.proj.bias, backbone.layers.9.ln2.weight, backbone.layers.9.ln2.bias, backbone.layers.9.ffn.layers.0.0.weight, backbone.layers.9.ffn.layers.0.0.bias, backbone.layers.9.ffn.layers.1.weight, backbone.layers.9.ffn.layers.1.bias, backbone.layers.10.ln1.weight, backbone.layers.10.ln1.bias, backbone.layers.10.attn.qkv.weight, backbone.layers.10.attn.qkv.bias, backbone.layers.10.attn.proj.weight, backbone.layers.10.attn.proj.bias, backbone.layers.10.ln2.weight, backbone.layers.10.ln2.bias, backbone.layers.10.ffn.layers.0.0.weight, backbone.layers.10.ffn.layers.0.0.bias, backbone.layers.10.ffn.layers.1.weight, backbone.layers.10.ffn.layers.1.bias, backbone.layers.11.ln1.weight, backbone.layers.11.ln1.bias, backbone.layers.11.attn.qkv.weight, backbone.layers.11.attn.qkv.bias, backbone.layers.11.attn.proj.weight, backbone.layers.11.attn.proj.bias, backbone.layers.11.ln2.weight, backbone.layers.11.ln2.bias, backbone.layers.11.ffn.layers.0.0.weight, backbone.layers.11.ffn.layers.0.0.bias, backbone.layers.11.ffn.layers.1.weight, backbone.layers.11.ffn.layers.1.bias, backbone.layers.12.ln1.weight, backbone.layers.12.ln1.bias, backbone.layers.12.attn.qkv.weight, backbone.layers.12.attn.qkv.bias, backbone.layers.12.attn.proj.weight, backbone.layers.12.attn.proj.bias, backbone.layers.12.ln2.weight, backbone.layers.12.ln2.bias, backbone.layers.12.ffn.layers.0.0.weight, backbone.layers.12.ffn.layers.0.0.bias, backbone.layers.12.ffn.layers.1.weight, backbone.layers.12.ffn.layers.1.bias, backbone.layers.13.ln1.weight, backbone.layers.13.ln1.bias, backbone.layers.13.attn.qkv.weight, backbone.layers.13.attn.qkv.bias, backbone.layers.13.attn.proj.weight, backbone.layers.13.attn.proj.bias, backbone.layers.13.ln2.weight, backbone.layers.13.ln2.bias, backbone.layers.13.ffn.layers.0.0.weight, backbone.layers.13.ffn.layers.0.0.bias, backbone.layers.13.ffn.layers.1.weight, backbone.layers.13.ffn.layers.1.bias, backbone.layers.14.ln1.weight, backbone.layers.14.ln1.bias, backbone.layers.14.attn.qkv.weight, backbone.layers.14.attn.qkv.bias, backbone.layers.14.attn.proj.weight, backbone.layers.14.attn.proj.bias, backbone.layers.14.ln2.weight, backbone.layers.14.ln2.bias, backbone.layers.14.ffn.layers.0.0.weight, backbone.layers.14.ffn.layers.0.0.bias, backbone.layers.14.ffn.layers.1.weight, backbone.layers.14.ffn.layers.1.bias, backbone.layers.15.ln1.weight, backbone.layers.15.ln1.bias, backbone.layers.15.attn.qkv.weight, backbone.layers.15.attn.qkv.bias, backbone.layers.15.attn.proj.weight, backbone.layers.15.attn.proj.bias, backbone.layers.15.ln2.weight, backbone.layers.15.ln2.bias, backbone.layers.15.ffn.layers.0.0.weight, backbone.layers.15.ffn.layers.0.0.bias, backbone.layers.15.ffn.layers.1.weight, backbone.layers.15.ffn.layers.1.bias, backbone.layers.16.ln1.weight, backbone.layers.16.ln1.bias, backbone.layers.16.attn.qkv.weight, backbone.layers.16.attn.qkv.bias, backbone.layers.16.attn.proj.weight, backbone.layers.16.attn.proj.bias, backbone.layers.16.ln2.weight, backbone.layers.16.ln2.bias, backbone.layers.16.ffn.layers.0.0.weight, backbone.layers.16.ffn.layers.0.0.bias, backbone.layers.16.ffn.layers.1.weight, backbone.layers.16.ffn.layers.1.bias, backbone.layers.17.ln1.weight, backbone.layers.17.ln1.bias, backbone.layers.17.attn.qkv.weight, backbone.layers.17.attn.qkv.bias, backbone.layers.17.attn.proj.weight, backbone.layers.17.attn.proj.bias, backbone.layers.17.ln2.weight, backbone.layers.17.ln2.bias, backbone.layers.17.ffn.layers.0.0.weight, backbone.layers.17.ffn.layers.0.0.bias, backbone.layers.17.ffn.layers.1.weight, backbone.layers.17.ffn.layers.1.bias, backbone.layers.18.ln1.weight, backbone.layers.18.ln1.bias, backbone.layers.18.attn.qkv.weight, backbone.layers.18.attn.qkv.bias, backbone.layers.18.attn.proj.weight, backbone.layers.18.attn.proj.bias, backbone.layers.18.ln2.weight, backbone.layers.18.ln2.bias, backbone.layers.18.ffn.layers.0.0.weight, backbone.layers.18.ffn.layers.0.0.bias, backbone.layers.18.ffn.layers.1.weight, backbone.layers.18.ffn.layers.1.bias, backbone.layers.19.ln1.weight, backbone.layers.19.ln1.bias, backbone.layers.19.attn.qkv.weight, backbone.layers.19.attn.qkv.bias, backbone.layers.19.attn.proj.weight, backbone.layers.19.attn.proj.bias, backbone.layers.19.ln2.weight, backbone.layers.19.ln2.bias, backbone.layers.19.ffn.layers.0.0.weight, backbone.layers.19.ffn.layers.0.0.bias, backbone.layers.19.ffn.layers.1.weight, backbone.layers.19.ffn.layers.1.bias, backbone.layers.20.ln1.weight, backbone.layers.20.ln1.bias, backbone.layers.20.attn.qkv.weight, backbone.layers.20.attn.qkv.bias, backbone.layers.20.attn.proj.weight, backbone.layers.20.attn.proj.bias, backbone.layers.20.ln2.weight, backbone.layers.20.ln2.bias, backbone.layers.20.ffn.layers.0.0.weight, backbone.layers.20.ffn.layers.0.0.bias, backbone.layers.20.ffn.layers.1.weight, backbone.layers.20.ffn.layers.1.bias, backbone.layers.21.ln1.weight, backbone.layers.21.ln1.bias, backbone.layers.21.attn.qkv.weight, backbone.layers.21.attn.qkv.bias, backbone.layers.21.attn.proj.weight, backbone.layers.21.attn.proj.bias, backbone.layers.21.ln2.weight, backbone.layers.21.ln2.bias, backbone.layers.21.ffn.layers.0.0.weight, backbone.layers.21.ffn.layers.0.0.bias, backbone.layers.21.ffn.layers.1.weight, backbone.layers.21.ffn.layers.1.bias, backbone.layers.22.ln1.weight, backbone.layers.22.ln1.bias, backbone.layers.22.attn.qkv.weight, backbone.layers.22.attn.qkv.bias, backbone.layers.22.attn.proj.weight, backbone.layers.22.attn.proj.bias, backbone.layers.22.ln2.weight, backbone.layers.22.ln2.bias, backbone.layers.22.ffn.layers.0.0.weight, backbone.layers.22.ffn.layers.0.0.bias, backbone.layers.22.ffn.layers.1.weight, backbone.layers.22.ffn.layers.1.bias, backbone.layers.23.ln1.weight, backbone.layers.23.ln1.bias, backbone.layers.23.attn.qkv.weight, backbone.layers.23.attn.qkv.bias, backbone.layers.23.attn.proj.weight, backbone.layers.23.attn.proj.bias, backbone.layers.23.ln2.weight, backbone.layers.23.ln2.bias, backbone.layers.23.ffn.layers.0.0.weight, backbone.layers.23.ffn.layers.0.0.bias, backbone.layers.23.ffn.layers.1.weight, backbone.layers.23.ffn.layers.1.bias, backbone.ln1.weight, backbone.ln1.bias, neck.mask_token, neck.decoder_pos_embed, neck.decoder_embed.weight, neck.decoder_embed.bias, neck.decoder_blocks.0.ln1.weight, neck.decoder_blocks.0.ln1.bias, neck.decoder_blocks.0.attn.qkv.weight, neck.decoder_blocks.0.attn.qkv.bias, neck.decoder_blocks.0.attn.proj.weight, neck.decoder_blocks.0.attn.proj.bias, neck.decoder_blocks.0.ln2.weight, neck.decoder_blocks.0.ln2.bias, neck.decoder_blocks.0.ffn.layers.0.0.weight, neck.decoder_blocks.0.ffn.layers.0.0.bias, neck.decoder_blocks.0.ffn.layers.1.weight, neck.decoder_blocks.0.ffn.layers.1.bias, neck.decoder_blocks.1.ln1.weight, neck.decoder_blocks.1.ln1.bias, neck.decoder_blocks.1.attn.qkv.weight, neck.decoder_blocks.1.attn.qkv.bias, neck.decoder_blocks.1.attn.proj.weight, neck.decoder_blocks.1.attn.proj.bias, neck.decoder_blocks.1.ln2.weight, neck.decoder_blocks.1.ln2.bias, neck.decoder_blocks.1.ffn.layers.0.0.weight, neck.decoder_blocks.1.ffn.layers.0.0.bias, neck.decoder_blocks.1.ffn.layers.1.weight, neck.decoder_blocks.1.ffn.layers.1.bias, neck.decoder_blocks.2.ln1.weight, neck.decoder_blocks.2.ln1.bias, neck.decoder_blocks.2.attn.qkv.weight, neck.decoder_blocks.2.attn.qkv.bias, neck.decoder_blocks.2.attn.proj.weight, neck.decoder_blocks.2.attn.proj.bias, neck.decoder_blocks.2.ln2.weight, neck.decoder_blocks.2.ln2.bias, neck.decoder_blocks.2.ffn.layers.0.0.weight, neck.decoder_blocks.2.ffn.layers.0.0.bias, neck.decoder_blocks.2.ffn.layers.1.weight, neck.decoder_blocks.2.ffn.layers.1.bias, neck.decoder_blocks.3.ln1.weight, neck.decoder_blocks.3.ln1.bias, neck.decoder_blocks.3.attn.qkv.weight, neck.decoder_blocks.3.attn.qkv.bias, neck.decoder_blocks.3.attn.proj.weight, neck.decoder_blocks.3.attn.proj.bias, neck.decoder_blocks.3.ln2.weight, neck.decoder_blocks.3.ln2.bias, neck.decoder_blocks.3.ffn.layers.0.0.weight, neck.decoder_blocks.3.ffn.layers.0.0.bias, neck.decoder_blocks.3.ffn.layers.1.weight, neck.decoder_blocks.3.ffn.layers.1.bias, neck.decoder_blocks.4.ln1.weight, neck.decoder_blocks.4.ln1.bias, neck.decoder_blocks.4.attn.qkv.weight, neck.decoder_blocks.4.attn.qkv.bias, neck.decoder_blocks.4.attn.proj.weight, neck.decoder_blocks.4.attn.proj.bias, neck.decoder_blocks.4.ln2.weight, neck.decoder_blocks.4.ln2.bias, neck.decoder_blocks.4.ffn.layers.0.0.weight, neck.decoder_blocks.4.ffn.layers.0.0.bias, neck.decoder_blocks.4.ffn.layers.1.weight, neck.decoder_blocks.4.ffn.layers.1.bias, neck.decoder_blocks.5.ln1.weight, neck.decoder_blocks.5.ln1.bias, neck.decoder_blocks.5.attn.qkv.weight, neck.decoder_blocks.5.attn.qkv.bias, neck.decoder_blocks.5.attn.proj.weight, neck.decoder_blocks.5.attn.proj.bias, neck.decoder_blocks.5.ln2.weight, neck.decoder_blocks.5.ln2.bias, neck.decoder_blocks.5.ffn.layers.0.0.weight, neck.decoder_blocks.5.ffn.layers.0.0.bias, neck.decoder_blocks.5.ffn.layers.1.weight, neck.decoder_blocks.5.ffn.layers.1.bias, neck.decoder_blocks.6.ln1.weight, neck.decoder_blocks.6.ln1.bias, neck.decoder_blocks.6.attn.qkv.weight, neck.decoder_blocks.6.attn.qkv.bias, neck.decoder_blocks.6.attn.proj.weight, neck.decoder_blocks.6.attn.proj.bias, neck.decoder_blocks.6.ln2.weight, neck.decoder_blocks.6.ln2.bias, neck.decoder_blocks.6.ffn.layers.0.0.weight, neck.decoder_blocks.6.ffn.layers.0.0.bias, neck.decoder_blocks.6.ffn.layers.1.weight, neck.decoder_blocks.6.ffn.layers.1.bias, neck.decoder_blocks.7.ln1.weight, neck.decoder_blocks.7.ln1.bias, neck.decoder_blocks.7.attn.qkv.weight, neck.decoder_blocks.7.attn.qkv.bias, neck.decoder_blocks.7.attn.proj.weight, neck.decoder_blocks.7.attn.proj.bias, neck.decoder_blocks.7.ln2.weight, neck.decoder_blocks.7.ln2.bias, neck.decoder_blocks.7.ffn.layers.0.0.weight, neck.decoder_blocks.7.ffn.layers.0.0.bias, neck.decoder_blocks.7.ffn.layers.1.weight, neck.decoder_blocks.7.ffn.layers.1.bias, neck.ln1.weight, neck.ln1.bias, neck.decoder_pred.weight, neck.decoder_pred.bias\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 1024 but got size 768 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# model = TimeSformer(img_size=224, num_classes=400, num_frames=8, attention_type='divided_space_time')\u001b[39;00m\n\u001b[1;32m     19\u001b[0m dummy_video \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m) \u001b[38;5;66;03m# (batch x channels x frames x height x width)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (2, 400)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/TimePSFormer/timesformer/models/vit.py:473\u001b[0m, in \u001b[0;36mTimePSformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 473\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/TimePSFormer/timesformer/models/vit.py:409\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 409\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/TimePSFormer/timesformer/models/vit.py:353\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    351\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# print(cls_tokens.shape, x.shape, T, W)\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m## resizing the positional embeddings in case they don't match the input at inference\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 1024 but got size 768 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/hongn/sapiens/pretrain')\n",
    "sys.path.append('/home/hongn/sapiens/pretrain/demo')\n",
    "from extract_feature import *\n",
    "\n",
    "import cv2\n",
    "import mmcv\n",
    "import mmengine\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from timesformer.models.vit import TimePSformer, TimeSformer\n",
    "\n",
    "model = TimePSformer(img_size=224, num_classes=400, num_frames=8, attention_type='time_only')\n",
    "# model = TimeSformer(img_size=224, num_classes=400, num_frames=8, attention_type='divided_space_time')\n",
    "\n",
    "dummy_video = torch.randn(2, 3, 8, 224, 224) # (batch x channels x frames x height x width)\n",
    "\n",
    "pred = model(dummy_video,) # (2, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train val test labels csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39805 38685\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "phase = 'test'\n",
    "train = pd.read_csv(f'/mnt/disks/persist2/kinetics400/annotations/{phase}.csv')\n",
    "id = train.youtube_id.to_list()\n",
    "train_path = glob.glob(f'/mnt/disks/persist2/kinetics400/{phase}/*.mp4')\n",
    "print(len(train), len(train_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(train_path)):\n",
    "    try:\n",
    "        idx = id.index(train_path[i].split('/')[-1][:-18])\n",
    "        labels.append(train.label[idx])\n",
    "    except:\n",
    "        print(f\"{train_path[i].split('/')[-1][:-18]} is not in the list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "\n",
    "data4timesformer = pd.DataFrame(\n",
    "    {'path': train_path,\n",
    "     'labels': labels,\n",
    "    })\n",
    "\n",
    "if phase == 'train':\n",
    "    label_encoder = preprocessing.LabelEncoder() \n",
    "    data4timesformer['labels']= label_encoder.fit_transform(data4timesformer['labels']) \n",
    "else:\n",
    "    data4timesformer['labels']= label_encoder.transform(data4timesformer['labels']) \n",
    "\n",
    "data4timesformer.to_csv(f'{phase}.csv', index=False, header=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify csv file because missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144375"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase = 'train'\n",
    "data = pd.read_csv(f'{phase}.csv', names=['paths', 'labels'])\n",
    "data.head()\n",
    "\n",
    "if phase == 'train':\n",
    "    data = data.drop(data.iloc[:,0].to_list().index('/mnt/disks/persist2/kinetics400/train/mf21EBmUtno_000151_000161.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f'{phase}.csv', index=False, header=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo cp /home/hongn/TimePSFormer/train.csv /mnt/disks/persist2/kinetics400/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase = 'train'\n",
    "# data = pd.read_csv(f'{phase}.csv')\n",
    "# data['labels'].unique() \n",
    "\n",
    "# from sklearn import preprocessing \n",
    "# label_encoder = preprocessing.LabelEncoder() \n",
    "# # Encode labels in column 'species'. \n",
    "# data['labels']= label_encoder.fit_transform(data['labels']) \n",
    "# data['labels'].unique() \n",
    "# data.to_csv(f'{phase}.csv', index=False, header=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>/mnt/disks/persist2/kinetics400/train/8mbN0ubh0q8_000000_000010.mp4</th>\n",
       "      <th>101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/disks/persist2/kinetics400/train/9bfc-7If...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/disks/persist2/kinetics400/train/VV0KA_f-...</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/disks/persist2/kinetics400/train/sWCnxZHX...</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/disks/persist2/kinetics400/train/Am6c9pVi...</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/disks/persist2/kinetics400/train/pC1hX1cP...</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  /mnt/disks/persist2/kinetics400/train/8mbN0ubh0q8_000000_000010.mp4  101\n",
       "0  /mnt/disks/persist2/kinetics400/train/9bfc-7If...                    17\n",
       "1  /mnt/disks/persist2/kinetics400/train/VV0KA_f-...                   167\n",
       "2  /mnt/disks/persist2/kinetics400/train/sWCnxZHX...                   318\n",
       "3  /mnt/disks/persist2/kinetics400/train/Am6c9pVi...                   356\n",
       "4  /mnt/disks/persist2/kinetics400/train/pC1hX1cP...                   231"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase = 'train'\n",
    "data = pd.read_csv(f'{phase}.csv')\n",
    "# data.to_csv(f'{phase}.csv', index=False, header=False)  \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144374"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:,0].to_list().index('/mnt/disks/persist2/kinetics400/train/mf21EBmUtno_000151_000161.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(162)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[144375,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo mv /mnt/disks/persist2/kinetics400/test.csv /home/hongn/TimePSFormer/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "224//16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo cp /home/hongn/TimePSFormer/val.csv /mnt/disks/persist2/kinetics400/val.csv\n",
    "sudo cp /home/hongn/TimePSFormer/train.csv /mnt/disks/persist2/kinetics400/train.csv\n",
    "sudo cp /home/hongn/TimePSFormer/test.csv /mnt/disks/persist2/kinetics400/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# pool of square window of size=3, stride=2\n",
    "stride = 64//14\n",
    "kernelsize = 64-(14-1)*stride\n",
    "m = nn.MaxPool2d(kernelsize, stride=stride)\n",
    "# pool of non-square window\n",
    "m = nn.MaxPool2d((kernelsize, kernelsize), stride=(stride, stride))\n",
    "input = torch.randn(20, 16, 64, 64)\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "14*14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 16, 14, 14])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "201326592/8192/8/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "201326592/8/3/2/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sapiens_lite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
